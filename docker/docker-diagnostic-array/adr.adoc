= ADR: diagnostics-array
:revdate: 2025-08-29
:status: Adopted

== Context
We need a container that runs network tests.  
It must discover, verify, and name targets.  
It must be secure.  
It must log.  
It must run fast without burning the host.  
It must be easy to add new tools.  
We also need a graph store to map relationships. Later we will add MAC, ARP, cabling, and physical sites.

== Decision
Build a FastAPI service named `diagnostics-array`.  
Start with three tools: *nmap*, *ping*, *dns*.  
They feed each other. They cover most ground.  
Design modular runners so new tools drop in clean.  
Add a graph database with CQL for assets and topology.  
API is run, baseline, compare.  
Tests run in parallel with caps.  
All endpoints are secured.  
All calls audited.  
Baselines stored as JSON for now; graph stores relationships.

== Order of Work
1. Implement nmap, ping, dns runners.  
2. Add GraphSink module.  
3. Upsert results into graph (opt-in).  
4. Chain outputs: nmap → ping → dns.  
5. Add suites for parallel runs.  
6. Add modular registry for new tools.  
7. Later add curl/http runner.  
8. Later import ARP, MAC, cabling, physical sites.

== Requirements

=== Functional
* Endpoints:
** `/health`  
** `/tests/capabilities`  
** `/tests/scan/nmap-{tcp,udp,custom}/{run,baseline,compare}`  
** `/tests/ping/{icmpv4,icmpv6,tcp}/{run,baseline,compare}`  
** `/tests/dns/{a,aaaa,ptr,mx,txt}/{run,baseline,compare}`  
** `/tests/suites` (CRUD)  
** `/tests/suites/{id}/compare`  
* Tests:
** run → observation JSON.  
** baseline → save `{params, observation, expected}`.  
** compare → rerun, evaluate assertions, return OK/NOK.  
* Chaining:
** nmap hosts → ping → dns.  
* Modularity:
** Runners with a simple contract.  
** Registered at startup.  
** Core only dispatches and validates.  
* Graph:
** Optional persistence.  
** Nodes: Device, IP, Port, Service, DNSName, Subnet, Interface, MAC, Location, ScanRun.  
** Relationships: HAS_IF, HAS_IP, HAS_MAC, HAS_PORT, OFFERS, RESOLVES_TO, IN_SUBNET, LOCATED_AT, CONNECTED_TO, SAW.

=== Non-Functional
* Security by default.  
* Timeouts, size caps, safe flags.  
* Parallel but bounded.  
* Audit with correlation IDs.  
* Non-root container. Least caps.  
* Persistence durable across restarts.  
* Graph writes safe and idempotent.

== Interface
Observations:
* nmap: `{open_ports:[{ip,proto,port,service,state}]}`  
* ping: `{reachable, loss_pct, rtt_avg_ms}`  
* dns: `{answers:[...]}`  
Assertions: field + op + value.  
Ops: `== != < <= > >= contains regex`.  
Compare: `{overall:"OK"|"NOK", assertions:[...], observation, diff?}`

== Concurrency
Async with semaphore.  
`MAX_CONCURRENCY` global.  
Split pools: fast (ping,dns), heavy (nmap).  
Cap per suite.  
Tool timeouts enforced.

== Security
Auth: API keys (HMAC).  
AuthZ: scopes per family/action.  
IP allow-lists.  
Optional mTLS/JWT later.  
Whitelist nmap flags.  
Validate hosts and CIDRs.  
All endpoints locked.

== Audit
Middleware logs JSON lines:  
time, CID, IP, key, scopes, path, status, duration, headers (redacted), body preview, result hash.  
Rotate by size. Prefer stdout→collector later.

== Settings
Env vars:  
* `AUTH_ENABLED`  
* `API_KEY_SECRET`, `API_KEYS_FILE`  
* `DATA_DIR`  
* `AUDIT_DIR`, `MAX_AUDIT_BYTES`  
* `DEFAULT_TIMEOUT`  
* `MAX_OUTPUT_BYTES`  
* `MAX_CONCURRENCY`, `MAX_CONCURRENCY_PER_SUITE`  
* `FAST_POOL_SIZE`, `HEAVY_POOL_SIZE`  
* `RATE_LIMIT_RPS`, `RATE_LIMIT_BURST`  
* `ALLOWED_NMAP_FLAGS`  
* `GRAPH_ENABLED`, `GRAPH_URI`, `GRAPH_USER`, `GRAPH_PASS`, `GRAPH_TLS`  
* `GRAPH_WRITE_BATCH`, `GRAPH_LABEL_PREFIX`

== Persistence
Baselines: JSON in `/data`.  
Suites: JSON in `/data/tests`.  
Audit logs: JSONL in `/data/audit`.  
Plan SQLite for baselines.  
Graph holds topology and history.

== Deployment
Docker image with tools.  
Non-root user.  
Optional NET_RAW.  
Expose 8080.  
Mount `/data`.  
Scale behind ingress.  
Redis for rate limits when scaling out.

== Graph Model
Nodes: Device, IP, Port, Service, DNSName, Subnet, Interface, MAC, Location, ScanRun.  
Rels:  
* `(Device)-[:HAS_IF]->(Interface)`  
* `(Interface)-[:HAS_IP]->(IP)`  
* `(Interface)-[:HAS_MAC]->(MAC)`  
* `(IP)-[:HAS_PORT]->(Port)`  
* `(Port)-[:OFFERS]->(Service)`  
* `(DNSName)-[:RESOLVES_TO]->(IP)`  
* `(IP)-[:IN_SUBNET]->(Subnet)`  
* `(Device)-[:LOCATED_AT]->(Location)`  
* `(Interface)-[:CONNECTED_TO]->(Interface)`  
* `(:ScanRun)-[:SAW]->(...)`

Indexes: unique on IP.addr, MAC.addr, DNSName.name, Subnet.cidr, Device.key, Interface.if_key.  
Ingest uses MERGE.  
Set first_seen, last_seen.  
Batch writes.  
Fail soft if graph is down.

== Risks
* Scans abused. Guard with scopes.  
* Scans trigger IDS. Keep defaults mild.  
* Heavy scans choke host. Cap concurrency.  
* JSON files race. Use SQLite later.  
* In-memory rate limits break at scale. Use Redis.  
* Logs lost if disk fills. Stream out.  
* Graph upserts can duplicate. Pick good keys.  
* Plugins can crash. Sandbox them.

== Acceptance
* Every call needs a valid key and scope.  
* Suite runs in parallel. Time ≈ slowest test.  
* Rate limits enforced and logged.  
* Baseline→Compare returns OK/NOK.  
* nmap flags outside whitelist blocked.  
* Audit logs have correlation IDs.  
* Graph upserts succeed when enabled; API still works when disabled.  

== Problems (blunt)
* API keys leak. Move to OIDC/mTLS later.  
* JSON baselines fragile. Use DB.  
* Disk logs weak. Stream out.  
* Too many assertion ops cause brittle tests. Start small.  
* One fat image is wide attack surface. Split later.  
* NET_RAW often banned. Fall back to TCP scans.  
* Don’t drift into full monitoring. Keep scope tight.

== Why This Order
Discovery (nmap).  
Verification (ping).  
Identity (dns).  
Modularity early.  
Graph for relationships.  
Add http, arp, mac, cabling, sites later.  
Keep spine strong before adding muscle.

== Next Steps
Build v1 with nmap, ping, dns runners.  
Implement runner registry.  
Add GraphSink.  
Wire chaining and suites.  
Keep assertions simple.  
Split pools.  
Audit to stdout optional.  
Plan SQLite + Redis.  
Add http and ARP/MAC imports later.